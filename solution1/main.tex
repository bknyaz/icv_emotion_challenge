\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{url}
\title{Emotion challenge fact sheet}
\author{iCV}
\date{February 2017}

\begin{document}

\maketitle

\section{Team details}

\begin{itemize}
\item Team name: NTechLab                                  
\item Team leader name: Boris Knyazev                           
\item Team leader address, phone number and email: 

Moscow, Russia

\texttt{borknyaz@gmail.com}

\item Rest of the team members: \textit{none}                    
\item Team website URL (if any): \url{https://github.com/bknyaz}
\item Affiliation: NTechLab
\end{itemize}

\section{Contribution details}

\begin{itemize}
\item Title of the contribution: \textbf{Simple unsupervised learning method}
\item Final score: \textit{to appear} (best validation score: 84.06 \%)                                             
\item General method description: This result is obtained based on the model described in [1]. 
The model is a convolutional neural network, filters of which are trained layer-wise using k-means clustering, that is without backpropagation. 
%Recursive autoconvolution is applied to image patches before $k$-means, which improves results. 
A linear SVM is applied on top of extracted features. In this challenge, a committee of 5 models with a single convolutional layer of 1024 filters turned out to work well.
The task of this challenge is treated as a standard image classification problem with 50 image classes.

\item References

[1] B. Knyazev, E. Barth and Thomas Martinetz, "Recursive Autoconvolution for Unsupervised Learning of Convolutional Neural Networks", accepted to IJCNN-2017
\item Representative image / diagram of the method        
\item Describe data preprocessing techniques applied (if any): 
faces are extracted and aligned using dlib (\url{https://github.com/davisking/dlib}). 
Images are downsized before face extraction to speed up the process.
\end{itemize}


\section{Face Landmarks Detection}
\subsection{Features / Data representation}
%Describe features used or data representation model FOR FACE LANDMARKS DETECTION (if any)

68 face landmarks are extracted by dlib, they are used to align faces.

\subsection{Dimensionality reduction}
%Dimensionality reduction technique applied FOR FACE LANDMARKS DETECTION (if any)

\subsection{Compositional model}
%Compositional model used, i.e. pictorial structure FOR FACE LANDMARKS DETECTION (if any)

\subsection{Learning strategy}
%Learning strategy applied FOR FACE LANDMARKS DETECTION (if any)

A publicly available Face Landmarks model is used: \url{http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2}.

\subsection{Other techniques}
%Other technique/strategy used not included in previous items FOR FACE LANDMARKS DETECTION (if any)

\subsection{Method complexity}
%Method complexity FOR FACE LANDMARKS DETECTION


\section{Dominant emotion recognition}

We treat all 50 emotions as independent (i.e. each image has one of the labels 0-49), so prior knowledge that there are dominant and complementary emotions was ignored.
We chose this approach, because: 

1) it's the most straightforward solution;

2) we tried to use prior knowledge and built some combinations of basic emotions and it did not work better in our case.

\subsection{Features / Data representation}
%Describe features used or data representation model FOR DOMINANT EMOTION RECOGNITION (if any)

\subsection{Dimensionality reduction}
%Dimensionality reduction technique applied FOR DOMINANT EMOTION RECOGNITION (if any)

\subsection{Compositional model}
%Compositional model used, i.e. pictorial structure FOR DOMINANT EMOTION RECOGNITION (if any)

\subsection{Learning strategy}
%Learning strategy applied FOR DOMINANT EMOTION RECOGNITION (if any)

\subsection{Other techniques}
%Other technique/strategy used not included in previous items FOR DOMINANT EMOTION RECOGNITION (if any)

\subsection{Method complexity}
%Method complexity FOR DOMINANT EMOTION RECOGNITION


\section{Complementary emotion recognition}
\subsection{Features / Data representation}
%Describe features used or data representation model FOR COMPLEMENTARY EMOTION RECOGNITION (if any)

\subsection{Dimensionality reduction}
%Dimensionality reduction technique applied FOR COMPLEMENTARY EMOTION RECOGNITION (if any)

\subsection{Compositional model}
%Compositional model used, i.e. pictorial structure FOR COMPLEMENTARY EMOTION RECOGNITION (if any)

\subsection{Learning strategy}
%Learning strategy applied FOR COMPLEMENTARY EMOTION RECOGNITION (if any)

\subsection{Other techniques}
%Other technique/strategy used not included in previous items FOR COMPLEMENTARY EMOTION RECOGNITION (if any)

\subsection{Method complexity}
%Method complexity FOR COMPLEMENTARY EMOTION RECOGNITION


\section{Joint dominant and complementary emotion recognition}
\subsection{Features / Data representation}
%Describe features used or data representation model FOR JOINT DOMINANT AND COMPLEMENTARY EMOTION RECOGNITION (if any)
Features are extracted using a single layer convolutional network consisting of a batch-norm layer, convolutional layer with 1024 filters, max-pooling, a ReLU rectifier, rootsift normalization. 
The input to the network is a $96\times96\times3$ aligned face image, max-pooling is performed for $12\times12$ non-overlapping regions, so that the feature vector is $8\times8\times1024=65536$ dimensional.
Filters are trained with $k$-means on ZCA-whitened image patches. Features are normalized using rootsift normalization (see [1] for details).

\subsection{Dimensionality reduction}
%Dimensionality reduction technique applied FOR JOINT DOMINANT AND COMPLEMENTARY EMOTION RECOGNITION (if any)
Principal component analysis (PCA) is applied to extracted features with the number of principal components equal to 500. 

We then take 10 subsets from 500 dimensional features. 
In the first subset there are features projected on the first 50 principal components, in the second subset there are features projected onto the first 100 principal components, and so on up to 500. 
So, instead of training just one SVM model for 500 dimensional feature vectors, we train 10 SVM models for different features.

\subsection{Compositional model}
%Compositional model used, i.e. pictorial structure FOR JOINT DOMINANT AND COMPLEMENTARY EMOTION RECOGNITION (if any)

\subsection{Learning strategy}
Data augmentation in the form of horizontal flipping is applied both for training and test samples, so that there are two times more images than provided.
A linear SVM is trained on projected features obtained with PCA.

We train filters 5 times and train 10 SVM models in each case (for dimensionalities 50-500), so that there 50 SVM models in total. Final predictions are obtained by averaging SVM scores.
%Learning strategy applied FOR JOINT DOMINANT AND COMPLEMENTARY EMOTION RECOGNITION (if any)

\subsection{Other techniques}
%Other technique/strategy used not included in previous items FOR JOINT DOMINANT AND COMPLEMENTARY EMOTION RECOGNITION (if any)

\subsection{Method complexity}
%Method complexity FOR JOINT DOMINANT AND COMPLEMENTARY EMOTION RECOGNITION


\section{Global Method Description}

\begin{itemize}
\item Total method complexity: 

The main model components are filters, which have $1024\times3\times15\times15=691200$ parameters, the PCA matrix with $65536 \times 500\approx33\cdot10^6$ parameters and SVM models with about $0.1\cdot10^6$ parameters.

\item Which pre-trained or external methods have been used (for any stage, if any): 

a dlib shape model to find Face Landmarks and align faces.

\item Which additional data has been used in addition to the provided training and validation data (at any stage, if any): 

filter training and PCA were performed on the combination of training, validation and test data.

\item Qualitative advantages of the proposed solution: 

- no external data and no pre-trained models are used other than the dlib shape model, 

- training and inference can be performed relatively fast,

- the model is simple and general purpose, so there are only few parameters that were  tuned for this task to obtain relatively good classification results.

\item Results of the comparison to other approaches (if any): \textit{none}

\item Novelty degree of the solution and if is has been previously published: the paper describing the model is accepted to IJCNN-2017 [1].
\end{itemize}

\section{Other details}

\begin{itemize}
\item Language and implementation details (including platform, memory, parallelization requirements): 

My testing environment is following:

- Ubuntu 16.04 LTS

- Matlab R2015b

- 32GB RAM

- Xeon CPU E5-2620 v3 @ 2.40GHz

- Optional: NVIDIA GTX 980 Ti

\item Detailed list of prerequisites for compilation

- gcc 5.4.0 (in my case)

- dlib

- MatConvNet

- VLFeat

- Liblinear

- Optional: CUDA 7.5, cuDNN-v5

\item Human effort required for implementation, training and validation?: 

1 person and about 6 hours

\item Training/testing expended time? 

3-4 hours for face extraction and 1-2 hours for model training

\item General comments and impressions of the challenge?

Provided images could have smaller size to make it easier to download and process them.

\end{itemize}
\end{document}
